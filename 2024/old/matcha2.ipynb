{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T16:07:30.980537Z","iopub.status.busy":"2023-12-24T16:07:30.979574Z","iopub.status.idle":"2023-12-24T16:07:30.988685Z","shell.execute_reply":"2023-12-24T16:07:30.987309Z","shell.execute_reply.started":"2023-12-24T16:07:30.980495Z"},"trusted":true},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from huggingface_hub import login\n","import time\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n","import wandb\n","import sys\n","import pdb\n","#from Levenshtein import distance as levenshtein_distance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T16:07:30.992334Z","iopub.status.busy":"2023-12-24T16:07:30.991435Z","iopub.status.idle":"2023-12-24T16:07:31.001000Z","shell.execute_reply":"2023-12-24T16:07:30.999909Z","shell.execute_reply.started":"2023-12-24T16:07:30.992305Z"},"trusted":true},"outputs":[],"source":["EPOCHS = 1\n","EVAL_STEP = 5\n","MODEL = \"hhhh\"\n","MAX_PATCHES = 1024\n","\n","#TRAIN_MODEL = \"martinsinnona/_test2\"\n","TRAIN_MODEL = \"google/matcha-base\"\n","#TRAIN_MODEL = \"eisenjulian/matcha-deplot-v2\"\n","\n","UPLOAD_METRICS = EPOCHS != -1\n","max_accuracy_test = 0.99\n","\n","PATCHES = []\n","TOKEN_OUTS = []\n","CAPTION_OUTS = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T16:07:31.005843Z","iopub.status.busy":"2023-12-24T16:07:31.005328Z","iopub.status.idle":"2023-12-24T16:08:24.345297Z","shell.execute_reply":"2023-12-24T16:08:24.344347Z","shell.execute_reply.started":"2023-12-24T16:07:31.005810Z"},"trusted":true},"outputs":[],"source":["login(token = \"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\")\n","\n","seed = 14895215085708117999\n","torch.manual_seed(seed)\n","\n","visdecode_dataset_train = load_dataset(\"martinsinnona/visdecode\", split = \"train\")\n","visdecode_dataset_test = load_dataset(\"martinsinnona/visdecode\", split = \"test\")\n","visdecode_dataset_test2 = load_dataset(\"martinsinnona/visdecode_web\", split = \"test\")\n","visdecode_dataset_test3 = load_dataset(\"martinsinnona/plotqa\", split = \"test\")\n","\n","if UPLOAD_METRICS and False:\n","\n","    wandb.login(key = \"451637d95c22df4568c6f5a268e37071bc14547b\")\n","    wandb.init(\n","        project=\"visdecode\", \n","        entity=\"martinsinnona\", \n","        config = {}\n","    )\n","    \n","print(\"\\nTRAIN:\\n\", visdecode_dataset_train, \"\\nTEST:\\n\", visdecode_dataset_test, \"\\nWEB:\\n\", visdecode_dataset_test2,\"\\n\")\n","\n","def compare_strings(str1, str2):\n","\n","    dmax = max(len(str1), len(str2), 1)\n","    #d = (levenshtein_distance(str1,str2))\n","    d = 1\n","\n","    return 1 - d / dmax\n","\n","def get_mark_type(str):\n","    \n","    start = str.find(\"<mark>\")\n","    end = str.find(\"</mark>\")\n","    \n","    if start != -1 and end != -1: return str[start+6:end]\n","    return \"\"\n","\n","def get_var_types(str):\n","    \n","    start1 = str.find(\"<type>\")\n","    end1 = str.find(\"</type>\")\n","    \n","    if start1 != -1 and end1 != -1: \n","        \n","        start2 = str.find(\"<type>\", end1+1)\n","        end2 = str.find(\"</type>\", end1+1)\n","        \n","        return str[start1+6:end1], str[start2+6:end2]\n","    \n","    return \"\",\"\"\n","\n","def get_var_names(str):\n","    \n","    start1 = str.find(\"<field>\")\n","    end1 = str.find(\"</field>\")\n","    \n","    if start1 != -1 and end1 != -1: \n","        \n","        start2 = str.find(\"<field>\", end1+1)\n","        end2 = str.find(\"</field>\", end1+1)\n","        \n","        return str[start1+7:end1], str[start2+7:end2]\n","    \n","    return \"\",\"\"\n","\n","class ImageCaptioningDataset(Dataset):\n","\n","    def __init__(self, dataset, processor):\n","        self.dataset = dataset\n","        self.processor = processor\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","\n","        item = self.dataset[idx]\n","        encoding = self.processor(images=item[\"image\"], text = \"\", return_tensors=\"pt\", add_special_tokens=True, max_patches=MAX_PATCHES)\n","\n","        encoding = {k:v.squeeze() for k,v in encoding.items()}\n","        encoding[\"text\"] = item[\"text\"]\n","\n","        return encoding\n","    \n","processor = AutoProcessor.from_pretrained(\"google/matcha-base\") # ybelkada/pix2struct-base\n","processor.image_processor.is_vqa = False\n","\n","model_name = (\"martinsinnona/\" + MODEL) if EPOCHS == -1 else TRAIN_MODEL\n","model = Pix2StructForConditionalGeneration.from_pretrained(model_name) \n","\n","print(\"\\n --------> using model base:\", model_name, \"<--------\\n\")\n","\n","def collator(batch):\n","\n","    new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n","    texts = [item[\"text\"] for item in batch]\n","\n","    text_inputs = processor(text=texts, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True, max_length=200)\n","\n","    new_batch[\"labels\"] = text_inputs.input_ids\n","\n","    for item in batch:\n","        new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n","        new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n","\n","    new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n","    new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n","\n","    return new_batch\n","\n","train_dataset = ImageCaptioningDataset(visdecode_dataset_train, processor)\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=collator)\n","\n","def compute_accuracies(results, output_raw = False):\n","    \n","    accuracies_mark_type = []\n","    accuracies_var_types = []\n","    accuracies_var_names_x = []\n","    accuracies_var_names_y = []\n","\n","    for res in results:\n","\n","        accuracy_mark_type = get_mark_type(res[0]) == get_mark_type(res[1])\n","        accuracy_var_types = get_var_types(res[0]) == get_var_types(res[1])\n","\n","        metadata_var_names = get_var_names(res[0])\n","        var_names = get_var_names(res[1])\n","\n","        accuracy_var_names_x = compare_strings(metadata_var_names[0], var_names[0])\n","        accuracy_var_names_y = compare_strings(metadata_var_names[1], var_names[1])\n","\n","        accuracies_mark_type.append(accuracy_mark_type)\n","        accuracies_var_types.append(accuracy_var_types)\n","        \n","        accuracies_var_names_x.append(accuracy_var_names_x)\n","        accuracies_var_names_y.append(accuracy_var_names_y)\n","\n","    accuracy_mark_type = np.round(np.mean(accuracies_mark_type), 2)\n","    accuracy_var_types = np.round(np.mean(accuracies_var_types), 2)\n","    \n","    accuracy_var_names_x = np.round(sum(1 for acc in accuracies_var_names_x if acc >= 0.7) / len(accuracies_var_names_x), 2)\n","    accuracy_var_names_y = np.round(sum(1 for acc in accuracies_var_names_y if acc >= 0.7) / len(accuracies_var_names_y), 2)\n","    \n","    if output_raw: return accuracies_mark_type, accuracies_var_types, accuracies_var_names_x, accuracies_var_names_y\n","    return accuracy_mark_type, accuracy_var_types, accuracy_var_names_x, accuracy_var_names_y\n","\n","def eval_model(dataset, print_output = False, raw_output = False):\n","    \n","    results = []\n","    i = 0\n","    \n","    for data in dataset:\n","        \n","        if i % 10 == 0: print(\"Evaluating...\", str(i))\n","        i += 1\n","        \n","        image = data[\"image\"]\n","\n","        model.eval()\n","        inputs = processor(images=image, return_tensors=\"pt\", max_patches=MAX_PATCHES).to(device)\n","\n","        flattened_patches = inputs.flattened_patches\n","        attention_mask = inputs.attention_mask\n","\n","        generated_ids = model.generate(flattened_patches=flattened_patches, attention_mask=attention_mask, max_length=200)\n","        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","        PATCHES.append(flattened_patches[0])\n","        TOKEN_OUTS.append(generated_ids)\n","        CAPTION_OUTS.append(generated_caption)\n","\n","        results.append([data[\"text\"], generated_caption])\n","\n","    print(\"\\n\")\n","\n","    if print_output:\n","        for res in results: \n","            print(res[0])\n","            print(res[1])\n","            print('\\n')\n","            \n","    acc_mark_type, acc_var_types, acc_var_names_x, acc_var_names_y = compute_accuracies(results, raw_output)\n","    \n","    print(\"\\n---------------- RESULTS --------------------\\n\")\n","    print(\"accuracy mark type :\", acc_mark_type)\n","    print(\"accuracy var types :\", acc_var_types)\n","    print(\"accuracy var names (x) (> 0.7) :\", acc_var_names_x)\n","    print(\"accuracy var names (y) (> 0.7) :\", acc_var_names_y)\n","    print(\"\\n---------------------------------------------\\n\")\n","    \n","    return acc_mark_type, acc_var_types, acc_var_names_x, acc_var_names_y\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5) #1e-5\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","model.train()\n","\n","losses = []\n","accuracies_train = []\n","accuracies_test = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-24T16:08:24.347886Z","iopub.status.busy":"2023-12-24T16:08:24.347362Z","iopub.status.idle":"2023-12-24T16:08:26.195785Z","shell.execute_reply":"2023-12-24T16:08:26.194271Z","shell.execute_reply.started":"2023-12-24T16:08:24.347851Z"},"trusted":true},"outputs":[],"source":["# ---------------------------------------- TRAINING ----------------------------------------------\n","\n","PATCHES = []\n","TOKEN_OUTS = []\n","CAPTION_OUTS = []\n","\n","for epoch in range(EPOCHS + 1):\n","\n","    if epoch == 0: start_time = time.time()\n","    print(\"Epoch: \", epoch)\n","    \n","    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=collator)\n","\n","    for idx, batch in enumerate(train_dataloader):\n","\n","        labels = batch.pop(\"labels\").to(device)\n","        flattened_patches = batch.pop(\"flattened_patches\").to(device)\n","        attention_mask = batch.pop(\"attention_mask\").to(device)\n","\n","        outputs = model(flattened_patches = flattened_patches,\n","                    attention_mask = attention_mask,\n","                    labels = labels)\n","\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","    if epoch == 0:\n","        \n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","        predicted_time = (elapsed_time * (EPOCHS+1)) / 3600\n","\n","        print(\"\\nAproximadamente quedan: \" + str(round(predicted_time,2)) + \" horas.\\n\")\n","    \n","    if epoch % EVAL_STEP == 0:\n","        \n","        #a, b, acc_var_names_train = eval_model(visdecode_dataset_train)\n","        acc_mark_type1, acc_var_types1, acc_var_names1_x, acc_var_names1_y  = eval_model(visdecode_dataset_test, print_output = True)\n","        #acc_mark_type2, acc_var_types2, acc_var_names2_x, acc_var_names2_y = eval_model(visdecode_dataset_test2, print_output = True)\n","\n","        accuracy_test = acc_var_names1_y   # CRITERIO PARA PUSHEAR A HUGGING FACE <----------------------------------------------------------\n","            \n","        if accuracy_test > max_accuracy_test:\n","            \n","            model.push_to_hub(MODEL)\n","            max_accuracy_test = accuracy_test\n","\n","        accuracies_test.append(accuracy_test)\n","        print(\"\\naccuracies test: \", accuracies_test,\"\\n\")\n","\n","        if UPLOAD_METRICS: \n","            \n","            #wandb.log({\"var_names_train\": acc_var_names_train})\n","            wandb.log({\"mark_type\": acc_mark_type1, \"var_types\": acc_var_types1, \"var_names_x\": acc_var_names1_x, \"var_names_y\": acc_var_names1_y})\n","            #wandb.log({\"mark_type_web\": acc_mark_type2, \"var_types_web\": acc_var_types2, \"var_names_web\": acc_var_names2})\n","\n","    losses.append(loss.cpu().detach().numpy().item())\n","    if UPLOAD_METRICS: wandb.log({\"loss\": loss.cpu().detach().numpy().item()})\n","\n","print(\"\\n------------------------------------------------------------------------------------------------------------------------\\n\")\n","\n","eval_model(visdecode_dataset_test, print_output = True)\n","eval_model(visdecode_dataset_test2, print_output = True)\n","#eval_model(visdecode_dataset_test3, print_output = True)\n","\n","if UPLOAD_METRICS: wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PATCHES[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","plt.imshow(PATCHES[0][400:500,100:400])\n","#plt.imshow(PATCHES[0])\n","plt.colorbar()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","plt.imshow(PATCHES[1][400:500,100:400])\n","plt.colorbar()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30554,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
