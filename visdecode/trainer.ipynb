{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from colors import *\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n",
    "import wandb\n",
    "import visdecode\n",
    "from visdecode import *\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_ID = 1\n",
    "\n",
    "MODEL = \"plotqa_simple_5\"\n",
    "TRAIN_MODEL = \"matcha-base\"\n",
    "COMMENT = \"\"\n",
    "\n",
    "UPLOAD_METRICS = True\n",
    "\n",
    "LR = 1e-5\n",
    "EPOCHS = 50\n",
    "EVAL_STEP = 5\n",
    "\n",
    "MAX_LENGTH = 100\n",
    "visdecode.MAX_LENGTH = MAX_LENGTH\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "MAX_SCORE = 0.6\n",
    "UPLOAD_METRICS = UPLOAD_METRICS and EPOCHS != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "device = \"cuda:\" + str(DEVICE_ID) if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key = \"451637d95c22df4568c6f5a268e37071bc14547b\")\n",
    "login(token = \"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\")\n",
    "\n",
    "dataset_train = load_dataset(\"martinsinnona/plotqa_simple_2k\", split = \"train\")\n",
    "\n",
    "dataset_val1 = load_dataset(\"martinsinnona/visdecode_simple_2k\", split = \"validation\")\n",
    "dataset_val2 = load_dataset(\"martinsinnona/plotqa_simple_2k\", split = \"validation\")\n",
    "\n",
    "dataset_test1 = load_dataset(\"martinsinnona/plotqa_simple_2k\", split = \"test\")\n",
    "dataset_test2 = load_dataset(\"martinsinnona/visdecode_web\", split = \"test\")\n",
    "\n",
    "print(bold(green(\"\\n[Train] :\")), len(dataset_train))\n",
    "\n",
    "print(bold(green(\"[Val] :\")), len(dataset_val1))\n",
    "print(bold(green(\"[Val #2] :\")), len(dataset_val2))\n",
    "\n",
    "print(bold(green(\"[Test #1] :\")), len(dataset_test1))\n",
    "print(bold(green(\"[Test #2] :\")), len(dataset_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL if EPOCHS == -1 else TRAIN_MODEL\n",
    "owner = \"martinsinnona\" if EPOCHS == -1 else \"google\"\n",
    "\n",
    "print(\"> Using model: \", bold(red(model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor, model = visdecode.load_model(owner, model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, processor, transform = None):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        image = item[\"image\"].convert(\"RGBA\")\n",
    "\n",
    "        if self.transform: \n",
    "             \n",
    "            image = self.transform(image)\n",
    "\n",
    "            white_background = Image.new(\"RGBA\", image.size, (255, 255, 255, 255))\n",
    "            image = Image.alpha_composite(white_background, image)\n",
    "\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        encoding = self.processor(images=image, text = \"\", return_tensors=\"pt\", add_special_tokens=True, max_patches=1024)\n",
    "\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        encoding[\"text\"] = item[\"text\"]\n",
    "\n",
    "        return encoding\n",
    "\n",
    "def collator(batch):\n",
    "\n",
    "    new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "\n",
    "    text_inputs = processor(text=texts, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    new_batch[\"labels\"] = text_inputs.input_ids\n",
    "\n",
    "    for item in batch:\n",
    "        new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n",
    "        new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n",
    "\n",
    "    new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n",
    "    new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomApply([transforms.RandomRotation(degrees = (0, 360), expand = True)], p=0),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(dataset_train, processor, transform)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = LR) \n",
    "model.to(device)\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPLOAD_METRICS:\n",
    "\n",
    "    wandb.init(\n",
    "        project = \"visdecode\",\n",
    "        name = MODEL, \n",
    "        entity = \"martinsinnona\", \n",
    "        config = {\"learning_rate\": LR,\n",
    "                  \"epochs\": EPOCHS,\n",
    "                  \"max_length\": MAX_LENGTH,\n",
    "                  \"batch_size\": BATCH_SIZE,\n",
    "                  \"comment\": COMMENT}\n",
    "    )\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS + 1):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "        labels = batch.pop(\"labels\").to(device)\n",
    "        flattened_patches = batch.pop(\"flattened_patches\").to(device)\n",
    "        attention_mask = batch.pop(\"attention_mask\").to(device)\n",
    "\n",
    "        outputs = model.forward(flattened_patches = flattened_patches, attention_mask = attention_mask, labels = labels)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim = -1)\n",
    "        \n",
    "        token_ids = torch.argmax(probs, dim = -1)\n",
    "        tokens = processor.batch_decode(token_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()     \n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.empty_cache()  \n",
    "        gc.collect() \n",
    "\n",
    "    print(bold(cyan(\"Epoch :\")), epoch, bold(green(\" | Loss :\")), loss.item())\n",
    "    if UPLOAD_METRICS: wandb.log({\"loss\":  loss.item()})\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # -------------------------------------\n",
    "\n",
    "    if epoch % EVAL_STEP == 0:\n",
    "\n",
    "        metrics_val1 = eval_model(processor, model, dataset_val1, device, vega_structure = False)   # visdecode dataset validation\n",
    "        metrics_val2 = eval_model(processor, model, dataset_val2, device, vega_structure = False)   # plotQA dataset validation\n",
    "\n",
    "        if metrics_val2[\"y_name\"] > MAX_SCORE:\n",
    "\n",
    "            model.push_to_hub(MODEL)\n",
    "            MAX_SCORE = metrics_val2[\"y_name\"]\n",
    "\n",
    "        if UPLOAD_METRICS: \n",
    "\n",
    "            wandb.log({\n",
    "                \n",
    "                    \"mark_type_val_1\":      metrics_val1[\"mark_type\"],\n",
    "                    \"x_type_val_1\":         metrics_val1[\"x_type\"], \n",
    "                    \"y_type_val_1\":         metrics_val1[\"y_type\"], \n",
    "                    \"x_name_val_1\":         metrics_val1[\"x_name\"],  \n",
    "                    \"y_name_val_1\":         metrics_val1[\"y_name\"],\n",
    "                    \"struct_error_val_1\":   metrics_val1[\"struct_error\"], \n",
    "\n",
    "                    \"mark_type_val_2\":      metrics_val2[\"mark_type\"],\n",
    "                    \"x_type_val_2\":         metrics_val2[\"x_type\"], \n",
    "                    \"y_type_val_2\":         metrics_val2[\"y_type\"],\n",
    "                    \"x_name_val_2\":         metrics_val2[\"x_name\"],\n",
    "                    \"y_name_val_2\":         metrics_val2[\"y_name\"], \n",
    "                    \"struct_error_val_2\":   metrics_val2[\"struct_error\"],\n",
    "\n",
    "                    \"epoch\": epoch})\n",
    "            \n",
    "if UPLOAD_METRICS: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(processor, model, dataset_test1, device, vega_structure = False)\n",
    "#eval_model(processor, model, dataset_test2, device, vega_structure = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "martin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
