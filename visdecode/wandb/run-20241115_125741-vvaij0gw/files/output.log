Traceback (most recent call last):
  File "/mnt/disk2/msinnona/visdecode/visdecode/trainer.py", line 150, in <module>
    outputs = model.forward(flattened_patches = flattened_patches, attention_mask = attention_mask, labels = labels)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/visdecode/visdecode/transformers/models/pix2struct/modeling_pix2struct.py", line 1706, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/visdecode/visdecode/transformers/models/pix2struct/modeling_pix2struct.py", line 629, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/visdecode/visdecode/transformers/models/pix2struct/modeling_pix2struct.py", line 354, in forward
    layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions) # su sumaron 4 GB
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/visdecode/visdecode/transformers/models/pix2struct/modeling_pix2struct.py", line 298, in forward
    self_attention_outputs = self.attention(                                # + 4GB VRAM
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/visdecode/visdecode/transformers/models/pix2struct/modeling_pix2struct.py", line 223, in forward
    attn_weights = nn.functional.softmax(scores, dim=-1, dtype=torch.float32).type_as(scores)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk2/msinnona/miniconda3/envs/martin/lib/python3.12/site-packages/torch/nn/functional.py", line 1860, in softmax
    ret = input.softmax(dim, dtype=dtype)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 735.44 MiB is free. Process 2093736 has 418.00 MiB memory in use. Process 3972472 has 418.00 MiB memory in use. Process 214665 has 42.26 GiB memory in use. Process 214896 has 7.04 GiB memory in use. Including non-PyTorch memory, this process has 23.29 GiB memory in use. Process 221035 has 5.08 GiB memory in use. Of the allocated memory 22.30 GiB is allocated by PyTorch, and 514.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)